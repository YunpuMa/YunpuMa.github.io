@comment{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  location={New Jersey},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  google_scholar_id={qyhmnyLat1gC},
  video={https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
  additional_info={. *More Information* can be [found here](https://github.com/alshedivat/al-folio/)},
  annotation={* Example use of superscripts<br>† Albert Einstein},
  inspirehep_id = {3255}
}



@misc{wang2025metokmultistageeventbasedtoken,
      title={METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding}, 
      author={Mengyue Wang and Shuo Chen and Kristian Kersting and Volker Tresp and Yunpu Ma†},
      year={2025},
      eprint={2506.02850},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.02850}, 

      preview={metok.png}, 
      pdf={https://arxiv.org/pdf/2506.02850},
      bibtex_show={true},
}

@misc{wang2025languagemixingreasoninglanguage,
      title={Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes}, 
      author={Mingyang Wang and Lukas Lange and Heike Adel and Yunpu Ma and Jannik Strötgen and Hinrich Schütze},
      year={2025},
      eprint={2505.14815},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.14815}, 

      preview={language_mixing.png}, 
      pdf={https://arxiv.org/pdf/2505.14815},
      bibtex_show={true},
}

@misc{bi2025cotkineticstheoreticalmodelingassessing,
      title={CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process}, 
      author={Jinhe Bi and Danqi Yan and Yifan Wang and Wenke Huang and Haokun Chen and Guancheng Wan and Mang Ye and Xun Xiao and Hinrich Schuetze and Volker Tresp and Yunpu Ma†},
      year={2025},
      eprint={2505.13408},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.13408}, 

      preview={cot_kinetics.png}, 
      pdf={https://arxiv.org/pdf/2505.13408},
      bibtex_show={true},
      selected={true},
}

@inproceedings{zhang2025webpilot,
  title={Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration},
  author={Zhang, Yao and Ma, Zijian and Ma†, Yunpu and Han, Zhen and Wu, Yu and Tresp, Volker},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={22},
  pages={23378--23386},
  year={2025}, 

  preview={webpilot.png}, 
  pdf={https://ojs.aaai.org/index.php/AAAI/article/download/34505/36660},
  bibtex_show={true},
  selected={true},
}

@misc{zhou2025opendrivevlaendtoendautonomousdriving,
      title={OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model}, 
      author={Xingcheng Zhou and Xuyuan Han and Feng Yang and Yunpu Ma and Alois C. Knoll},
      year={2025},
      eprint={2503.23463},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.23463}, 

      preview={opendrivevla.png}, 
      pdf={https://arxiv.org/pdf/2503.23463},
      bibtex_show={true},
      code={https://drivevla.github.io/}, 
}

@misc{he2025supposedlyequivalentfactsarent,
      title={Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs}, 
      author={Yuan He and Bailan He and Zifeng Ding and Alisia Lupidi and Yuqicheng Zhu and Shuo Chen and Caiqi Zhang and Jiaoyan Chen and Yunpu Ma and Volker Tresp and Ian Horrocks},
      year={2025},
      eprint={2503.22362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.22362}, 

      preview={entity_frequency.png}, 
      pdf={https://arxiv.org/pdf/2503.22362},
      bibtex_show={true},
}

@misc{bi2025prismselfpruningintrinsicselection,
      title={PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection}, 
      author={Jinhe Bi and Yifan Wang and Danqi Yan and Xun Xiao and Artur Hecker and Volker Tresp and Yunpu Ma†},
      year={2025},
      eprint={2502.12119},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.12119}, 

      preview={prism.png}, 
      pdf={https://arxiv.org/pdf/2502.12119},
      bibtex_show={true},
      selected={true},
}

@misc{bi2025llavasteeringvisualinstruction,
      title={LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering}, 
      author={Jinhe Bi and Yujun Wang and Haokun Chen and Xun Xiao and Artur Hecker and Volker Tresp and Yunpu Ma†},
      year={2025},
      booktitle={Annual Meeting of the Association for Computational Linguistics},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.12359}, 

      preview={llava_steering.png}, 
      pdf={https://arxiv.org/abs/2412.12359},
      bibtex_show={true},
      selected={true},
      code={https://github.com/bibisbar/LLaVA-Steering}, 
}

@misc{liu2024perftparameterefficientroutedfinetuning,
      title={PERFT: Parameter-Efficient Routed Fine-Tuning for Mixture-of-Expert Model}, 
      author={Yilun Liu* and Yunpu Ma*† and Shuo Chen and Zifeng Ding and Bailan He and Zhen Han and Volker Tresp},
      year={2024},
      eprint={2411.08212},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.08212}, 

      preview={perft.png}, 
      pdf={https://arxiv.org/abs/2411.08212},
      bibtex_show={true},
      selected={true},
}

@inproceedings{liao-etal-2024-videoinsta,
    title = "{V}ideo{INSTA}: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with {LLM}s",
    author = "Liao, Ruotong  and
      Erler, Max  and
      Wang, Huiyu  and
      Zhai, Guangyao  and
      Zhang, Gengyuan  and
      Ma†, Yunpu  and
      Tresp, Volker",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.384/",
    doi = "10.18653/v1/2024.findings-emnlp.384",
    pages = "6577--6602",
    abstract = "In the video-language domain, recent works in leveraging zero-shot Large Language Model-based reasoning for video understanding have become competitive challengers to previous end-to-end models. However, long video understanding presents unique challenges due to the complexity of reasoning over extended timespans, even for zero-shot LLM-based approaches. The challenge of information redundancy in long videos prompts the question of what specific information is essential for large language models (LLMs) and how to leverage them for complex spatial-temporal reasoning in long-form video analysis. We propose a framework VideoINSTA , i.e. INformative Spatial-TemporAl Reasoning for zero-shot long-form video understanding.VideoINSTA contributes (1) a zero-shot framework for long video understanding using LLMs; (2) an event-based temporalreasoning and content-based spatial reasoning approach for LLMs to reason over spatial-temporal information in videos; (3) a self-reflective information reasoning scheme based on information sufficiency and prediction confidence while balancing temporal factors.Our model significantly improves the state-of-the-art on three long video question-answering benchmarks: EgoSchema, NextQA, and IntentQA, and the open question answering dataset ActivityNetQA. Code is released: https://github.com/mayhugotong/VideoINSTA.", 

    preview={videoinsta.png}, 
    pdf={https://aclanthology.org/2024.findings-emnlp.384.pdf},
    bibtex_show={true},
}

@misc{ding2025dygmambaefficientlymodelinglongterm,
      title={DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models}, 
      author={Zifeng Ding and Yifeng Li and Yuan He and Antonio Norelli and Jingcheng Wu and Volker Tresp and Michael Bronstein and Yunpu Ma†},
      booktitle={Transactions on Machine Learning Research}, 
      year={2025},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.04713}, 

      preview={dygmamba.png}, 
      pdf={https://arxiv.org/pdf/2408.04713},
      bibtex_show={true},
}
